{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsO/Y+QGVswqzoOGhMiz+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/ML_task2/blob/main/ML_Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"><h1>**MACHINE LEARNING**</h1></font>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Aplicados al conjunto de datos original los siguientes procesos :\n",
        "\n",
        "1.   Pre-procesado.\n",
        "2.   Tokenización y lematización.\n",
        "3.   Eliminación de stopswords.\n",
        "\n",
        "El proceso se ha aplicado a los idiomas Español e Inglés por separado y generando dos archivos :\n",
        "\n",
        "\n",
        "\n",
        "*   clean_test_task2.csv\n",
        "*   clean_train_task2.csv\n",
        "\n",
        "\n",
        "\n",
        "El siguiente paso es aplicar un conjunto de algoritmos y evaluar los modelos obtenidos.\n",
        "\n",
        "Se aplican los siguientes algoritmos :\n",
        "\n",
        "1.   SVC.\n",
        "2.   Arbol de decisión.\n",
        "3.   XGBoost.\n",
        "4.   NB_Multinomial."
      ],
      "metadata": {
        "id": "Qax5mtERy0V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de las librerías y paquetes necesarios\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbqhEWocWLFp",
        "outputId": "3e8e571d-1fcb-4fd2-b89a-15c262d8c45e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de entrenamiento\n",
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"/content/EXIST2021_training.tsv\", delimiter='\\t', usecols=['language','text', 'task2'], quoting=3, error_bad_lines=False)\n",
        "test = pd.read_csv(\"/content/EXIST2021_test_labeled.tsv\", delimiter='\\t', usecols=['language','text', 'task2'], quoting=3, error_bad_lines=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-N7WY5yYM-W",
        "outputId": "e45a8933-93bf-4479-9059-448b79b8c8cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-113f6e7b2ced>:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  train = pd.read_csv(\"/content/EXIST2021_training.tsv\", delimiter='\\t', usecols=['language','text', 'task2'], quoting=3, error_bad_lines=False)\n",
            "<ipython-input-2-113f6e7b2ced>:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  test = pd.read_csv(\"/content/EXIST2021_test_labeled.tsv\", delimiter='\\t', usecols=['language','text', 'task2'], quoting=3, error_bad_lines=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBHYKYHEdHTE",
        "outputId": "1c2919ad-ec28-49d5-80ca-a5671cdd4234"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  language                                               text  \\\n",
            "0       en  Pennsylvania State Rep horrifies with opening ...   \n",
            "1       en  @iilovegrapes He sounds like as ass, and very ...   \n",
            "2       en  \"@averyangryskel1 @4ARealistParty LOL! \"\"This ...   \n",
            "3       en  @WanderOrange @stalliontwink Rights?I mean yea...   \n",
            "4       en  the jack manifold appreciation i’m seeing is o...   \n",
            "\n",
            "                    task2  \n",
            "0              non-sexist  \n",
            "1              non-sexist  \n",
            "2  ideological-inequality  \n",
            "3  ideological-inequality  \n",
            "4              non-sexist  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"brown\"><h1>**Preprocesado**</h1></font>\n",
        "\n",
        "\n",
        "\n",
        "1.   Para el idioma Inglés.\n",
        "2.   Para el idioma Español.\n",
        "3.   Concatenación de los data_frame de cada idioma preprocesado.\n",
        "\n"
      ],
      "metadata": {
        "id": "2hAYJ9P9zcib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-procesamiento par el idioma Inglés\n",
        "\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Función para mapear las etiquetas POS a las etiquetas de WordNet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Función para preprocesar el texto\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Eliminar usuarios identificados por @\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "\n",
        "    # Eliminar URL's y links\n",
        "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
        "\n",
        "    text = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', text)\n",
        "\n",
        "    # Elimina referencias link o video\n",
        "\n",
        "    text = re.sub(r'{link}', '', text)\n",
        "    text = re.sub(r\"\\[video\\]\", '', text)\n",
        "\n",
        "    # Elimina entidades HTML\n",
        "    text = re.sub(r'&[a-z]+;', '', text)\n",
        "\n",
        "    # Eliminar caracteres especiales y convertir a minúsculas\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "\n",
        "    # Elimnar los caracteres que no coincidan con el patrón\n",
        "    text = re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', text)\n",
        "\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Eliminar stopwords en inglés\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lematizar los tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in filtered_tokens]\n",
        "\n",
        "    # Unir los tokens procesados en un texto procesado\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Obtener la columna 'text' del DataFrame 'train' y 'test' donde 'language' es 'en'\n",
        "\n",
        "texts_train = train.loc[train['language'] == 'en', 'text']\n",
        "texts_test = test.loc[test['language'] == 'en', 'text']\n",
        "\n",
        "# Aplicar el preprocesamiento a cada texto en la columna 'text'\n",
        "preprocessed_texts_train = [preprocess_text(text) for text in texts_train]\n",
        "preprocessed_texts_test = [preprocess_text(text) for text in texts_test]\n",
        "\n",
        "# Crear un nuevo DataFrame con las columnas 'text' y 'task1' para train y test\n",
        "en_df_train = pd.DataFrame({'text': preprocessed_texts_train, 'task2': train.loc[train['language'] == 'en', 'task2']})\n",
        "en_df_test = pd.DataFrame({'text': preprocessed_texts_test, 'task2': test.loc[test['language'] == 'en', 'task2']})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oYF_4y-uPqj",
        "outputId": "f7622ea2-71ef-48ab-daa5-522593e4eae4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(en_df_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwjciL66dPS0",
        "outputId": "f0957e8d-4859-43d5-9594-6b57621dc4ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text                   task2\n",
            "0  call antifeminazi shut fuck vile commentary el...  ideological-inequality\n",
            "1  back woman brave beautiful bad as babe deserve...              non-sexist\n",
            "2                        wow skirt short length inch         objectification\n",
            "3  incredible beautifulbut laugh much read drift ...              non-sexist\n",
            "4  find extremely hard believe kelly yr old mum w...              non-sexist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-procesamiento para el idioma Español.\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "# Descargar el modelo de SpaCy para el idioma español\n",
        "spacy.cli.download(\"es_core_news_sm\")\n",
        "\n",
        "# Cargar el modelo de SpaCy para el idioma español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Resto del código...\n",
        "\n",
        "# Función para tokenizar y lematizar el texto utilizando SpaCy\n",
        "def preprocess_text(text):\n",
        "    # Eliminar usuarios identificados por @\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "\n",
        "    # Elimina URL's y links\n",
        "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
        "\n",
        "    text = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', text)\n",
        "\n",
        "    # Elimina referencias link o video\n",
        "\n",
        "    text = re.sub(r'{link}', '', text)\n",
        "    text = re.sub(r\"\\[video\\]\", '', text)\n",
        "\n",
        "    # Elimina entidades HTML\n",
        "    text = re.sub(r'&[a-z]+;', '', text)\n",
        "\n",
        "    # Eliminar caracteres especiales y convertir a minúsculas\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "\n",
        "    # Elimnar los caracteres que no coincidan con el patrón\n",
        "    text = re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', text)\n",
        "\n",
        "    # Tokenizar el texto utilizando SpaCy\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Eliminar stopwords en español\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Unir los tokens procesados en un texto procesado\n",
        "    processed_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "\n",
        "# Obtener la columna 'text' del DataFrame 'train' donde 'language' es 'es'\n",
        "texts_train = train[train['language'] == 'es']['text']\n",
        "texts_test = test[test['language'] == 'es']['text']\n",
        "\n",
        "# Aplicar el preprocesamiento a cada texto en la columna 'text'\n",
        "preprocessed_texts_train = [preprocess_text(text) for text in texts_train]\n",
        "preprocessed_texts_test = [preprocess_text(text) for text in texts_test]\n",
        "\n",
        "# Crear un nuevo DataFrame con la columna 'pre\n",
        "# Crear un nuevo DataFrame con las columnas 'text' y 'task1' para train y test\n",
        "es_df_train = pd.DataFrame({'text': preprocessed_texts_train, 'task2': train.loc[train['language'] == 'es', 'task2']})\n",
        "es_df_test = pd.DataFrame({'text': preprocessed_texts_test, 'task2': test.loc[test['language'] == 'es', 'task2']})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTt1zzIIwPYQ",
        "outputId": "3c6ced6f-c539-4f00-e509-57c6d95c5e06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(es_df_test.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPDyshNvdekw",
        "outputId": "61263d34-b9f8-4f37-ff73-f85b7741f544"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  \\\n",
            "2208                            golpear ms fuerte perra   \n",
            "2209     castigar tres mes salir chica venir casa notau   \n",
            "2210      jajajaj haber topar onda   hacer mansplaining   \n",
            "2211  resultar hoy ser da juventud quedar ms felicit...   \n",
            "2212    decir creer igualdad feminismo actual ser he...   \n",
            "\n",
            "                             task2  \n",
            "2208  misogyny-non-sexual-violence  \n",
            "2209                    non-sexist  \n",
            "2210        stereotyping-dominance  \n",
            "2211                    non-sexist  \n",
            "2212        ideological-inequality  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinación de los dataframes train y test de cada idioma\n",
        "combined_df_train = pd.concat([en_df_train, es_df_train])\n",
        "combined_df_train = combined_df_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "combined_df_test = pd.concat([en_df_test, es_df_test])\n",
        "combined_df_test = combined_df_test.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "EcFi8iXLdi_v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminación de palabras de 3 o menos caracteres\n",
        "import re\n",
        "\n",
        "# Definir la función de eliminación de palabras cortas\n",
        "def eliminar_palabras_cortas(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if len(word) > 3]\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "    return filtered_text\n",
        "\n",
        "# Aplicar la eliminación de palabras cortas a la columna 'text'\n",
        "combined_df_train['text'] = combined_df_train['text'].apply(lambda x: eliminar_palabras_cortas(x))\n",
        "combined_df_test['text'] = combined_df_test['text'].apply(lambda x: eliminar_palabras_cortas(x))"
      ],
      "metadata": {
        "id": "zQFxRu_Adnm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df_train.to_csv('clean_train_task2.csv', index=False, encoding='latin-1')"
      ],
      "metadata": {
        "id": "Llk-oDYbduFL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_df_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm1d6ABLdyz_",
        "outputId": "025777f6-44fb-4821-9aeb-06712b5a98b6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  libro prohibir manzana encontrar serpiente apr...   \n",
            "1  ser super dino hablar chicao alejense ver tene...   \n",
            "2  exactly actually grab woman pussy assume get y...   \n",
            "3  dontseeing teenager high as get gangbanged hum...   \n",
            "4    qu puta sacar maquillaje qu pasaaar jajaj ma...   \n",
            "\n",
            "                          task2  \n",
            "0        ideological-inequality  \n",
            "1                    non-sexist  \n",
            "2  misogyny-non-sexual-violence  \n",
            "3               sexual-violence  \n",
            "4        stereotyping-dominance  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"brown\"><h1>**Configuración**</h1></font>\n",
        "\n",
        "\n",
        "1.   Dividir los datos en texto y clases.\n",
        "2.   Convertir las etiquetas de la clase en valor númérico.\n",
        "3.   Vectorización Tf_Id para extraer las características del texto.\n",
        "\n"
      ],
      "metadata": {
        "id": "_kx1SVqEz8KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = combined_df_train['text']\n",
        "test_data = combined_df_test['text']\n",
        "Y_train = combined_df_train['task2']\n",
        "Y_test = combined_df_test['task2']"
      ],
      "metadata": {
        "id": "DIbuxNxod3-t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir las etiquetas en valores numéricos\n",
        "encoder = LabelEncoder()\n",
        "Y_train_encoded = encoder.fit_transform(Y_train)\n",
        "Y_test_encoded = encoder.fit_transform(Y_test)"
      ],
      "metadata": {
        "id": "6rmg6CZavFkk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data)\n",
        "X_test_vectorized = vectorizer.transform(test_data)\n"
      ],
      "metadata": {
        "id": "BnqfHoqjuCdx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"brown\"><h1>**Algoritmos**</h1></font>\n",
        "\n",
        "\n",
        "1.   Entrenamiento.\n",
        "2.   Predicción.\n",
        "3.   Evaluación del modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "6BVLe6fa0cHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"><h1>**SVC**</h1></font>\n",
        "\n",
        "1.   SVC con configuraciñon mínima.\n",
        "2.   SVC con ajuste de parámetros. Objetivo mejorar el modelo anterior.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Nfm86En65fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste mínimo\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train_vectorized, Y_train_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "jxXaJ6piuPZk",
        "outputId": "57acd30f-80d2-4138-b6f7-0b23cadb7072"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_vectorized)\n"
      ],
      "metadata": {
        "id": "_mawqK1ruaDW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(Y_test_encoded, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei85GPs7udCh",
        "outputId": "81f5bf1b-8a6b-43fe-ccbd-bb956dd2adff"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.29      0.43       621\n",
            "           1       0.64      0.12      0.21       472\n",
            "           2       0.53      0.97      0.69      2087\n",
            "           3       0.70      0.14      0.24       324\n",
            "           4       0.67      0.12      0.20       400\n",
            "           5       0.73      0.17      0.28       464\n",
            "\n",
            "    accuracy                           0.56      4368\n",
            "   macro avg       0.69      0.30      0.34      4368\n",
            "weighted avg       0.63      0.56      0.48      4368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustes de parámetros.\n",
        "# Definir los parámetros a ajustar\n",
        "model= SVC()\n",
        "parameters = {'C': [0.1, 1, 10],\n",
        "              'gamma': [0.1, 0.01, 0.001]}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Obtener la mejor configuración de parámetros encontrada\n",
        "best_params = grid_search.best_params_\n",
        "# Crear el modelo final con los mejores parámetros\n",
        "final_model = SVC(**best_params)\n",
        "final_model.fit(X_train_vectorized, Y_train_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Ry6-0_7b31fg",
        "outputId": "4f588ec9-93df-4f43-c450-a52601cccc2b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10, gamma=0.1)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, gamma=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, gamma=0.1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Realizar la predicción en el conjunto de prueba\n",
        "Y_pred = final_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calcular la precisión del modelo\n",
        "accuracy = accuracy_score(Y_test_encoded, Y_pred)\n",
        "\n",
        "# Imprimir la precisión del modelo\n",
        "print(\"Precisión del modelo: {:.2f}\".format(accuracy))\n",
        "\n",
        "# Generar el informe de clasificación\n",
        "classification_report = classification_report(Y_test_encoded, Y_pred)\n",
        "\n",
        "# Imprimir el informe de clasificación\n",
        "print(\"Informe de clasificación:\\n\", classification_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkQenCWO42Ui",
        "outputId": "aee04de4-d3c2-4513-e358-b06c452c4624"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión del modelo: 0.59\n",
            "Informe de clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.44      0.53       621\n",
            "           1       0.48      0.30      0.37       472\n",
            "           2       0.61      0.86      0.71      2087\n",
            "           3       0.52      0.28      0.37       324\n",
            "           4       0.58      0.35      0.44       400\n",
            "           5       0.52      0.31      0.39       464\n",
            "\n",
            "    accuracy                           0.59      4368\n",
            "   macro avg       0.56      0.42      0.47      4368\n",
            "weighted avg       0.58      0.59      0.56      4368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"><h1>**Random Forest**</h1></font>\n",
        "\n",
        "1.   Rf con configuraciñon mínima.\n",
        "2.   Rf con ajuste de parámetros. Objetivo mejorar el modelo anterior."
      ],
      "metadata": {
        "id": "RyCdkhnf7lm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rf con ajuste mínimo\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Crear el modelo de Random Forest\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Entrenar el modelo\n",
        "rf_model.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "Y_pred_rf = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(classification_report(Y_test_encoded, Y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTjW_7bOwwKV",
        "outputId": "d4d208c1-d4dd-40b3-f43e-c3d7b8fcf8bb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.43      0.54       621\n",
            "           1       0.49      0.22      0.31       472\n",
            "           2       0.57      0.91      0.70      2087\n",
            "           3       0.57      0.22      0.32       324\n",
            "           4       0.55      0.25      0.34       400\n",
            "           5       0.64      0.26      0.37       464\n",
            "\n",
            "    accuracy                           0.58      4368\n",
            "   macro avg       0.60      0.38      0.43      4368\n",
            "weighted avg       0.59      0.58      0.54      4368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rf con ajuste de parámetros\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definir el modelo Random Forest\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Definir los parámetros a ajustar\n",
        "parameters = {'n_estimators': [100, 200, 300],\n",
        "              'max_depth': [None, 5, 10],\n",
        "              'min_samples_split': [2, 5, 10],\n",
        "              'min_samples_leaf': [1, 2, 4]}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Obtener la mejor configuración de parámetros encontrada\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Crear el modelo final con los mejores parámetros\n",
        "final_model = RandomForestClassifier(**best_params)\n",
        "final_model.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "Y_pred_rf = final_model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(classification_report(Y_test_encoded, Y_pred_rf))\n"
      ],
      "metadata": {
        "id": "lcC8_nwh75CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "Y_pred_rf = final_model.predict(X_test_vectorized)\n",
        "print(classification_report(Y_test_encoded, Y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yG-YrL1Od2T",
        "outputId": "5d0027df-3361-4959-f305-f22cc18fe4dd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.46      0.58       621\n",
            "           1       0.54      0.22      0.32       472\n",
            "           2       0.57      0.91      0.70      2087\n",
            "           3       0.54      0.24      0.33       324\n",
            "           4       0.55      0.28      0.37       400\n",
            "           5       0.70      0.25      0.37       464\n",
            "\n",
            "    accuracy                           0.59      4368\n",
            "   macro avg       0.62      0.39      0.45      4368\n",
            "weighted avg       0.61      0.59      0.55      4368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"><h1>**Naive Bayes**</h1></font>\n",
        "\n",
        "1.   Nb con configuraciñon mínima.\n",
        "2.   Nb con ajuste de parámetros. Objetivo mejorar el modelo anterior."
      ],
      "metadata": {
        "id": "cj96f7OC7jU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb con ajuste mínimo.\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Crear el modelo de Multinomial Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Entrenar el modelo\n",
        "nb_model.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "Y_pred_nb = nb_model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(classification_report(Y_test_encoded, Y_pred_nb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a8bLrXLw1a-",
        "outputId": "c1ce34a7-55b3-46f8-b9a1-1f4e6f7acb5c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.01      0.02       621\n",
            "           1       1.00      0.00      0.01       472\n",
            "           2       0.48      1.00      0.65      2087\n",
            "           3       0.60      0.01      0.02       324\n",
            "           4       0.67      0.01      0.01       400\n",
            "           5       0.92      0.05      0.09       464\n",
            "\n",
            "    accuracy                           0.49      4368\n",
            "   macro avg       0.75      0.18      0.13      4368\n",
            "weighted avg       0.66      0.49      0.33      4368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NB con ajustes de parámetros\n",
        "# Definir los parámetros a ajustar\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Crear el modelo MultinomialNB\n",
        "model = MultinomialNB()\n",
        "\n",
        "parameters = {'alpha': [0.1, 0.5, 1.0]}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Obtener la mejor configuración de parámetros encontrada\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Crear el modelo final con los mejores parámetros\n",
        "final_model = MultinomialNB(**best_params)\n",
        "final_model.fit(X_train_vectorized, Y_train_encoded)\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "Y_pred_nb = final_model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(classification_report(Y_test_encoded, Y_pred_nb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ0ZM4SRQNhr",
        "outputId": "de20a846-673f-4479-cd84-d7466e7ce652"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.16      0.26       621\n",
            "           1       0.46      0.08      0.13       472\n",
            "           2       0.51      0.95      0.66      2087\n",
            "           3       0.65      0.10      0.18       324\n",
            "           4       0.61      0.11      0.18       400\n",
            "           5       0.54      0.13      0.21       464\n",
            "\n",
            "    accuracy                           0.52      4368\n",
            "   macro avg       0.57      0.26      0.27      4368\n",
            "weighted avg       0.54      0.52      0.42      4368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"><h1>**XGBoost**</h1></font>\n",
        "\n",
        "1.   Xg con configuraciñon mínima.\n",
        "2.   Xg con ajuste de parámetros. Objetivo mejorar el modelo anterior."
      ],
      "metadata": {
        "id": "KeGlGYm6SLEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost con ajuste mínimo\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Crear el modelo de XGBoost\n",
        "xgb_model = XGBClassifier()\n",
        "\n",
        "# Entrenar el modelo\n",
        "xgb_model.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "Y_pred_xgb = xgb_model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(classification_report(Y_test_encoded, Y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmlLPXxoxil9",
        "outputId": "2d53c795-e503-44b2-a1b3-05b773466c88"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.47      0.58       621\n",
            "           1       0.50      0.25      0.34       472\n",
            "           2       0.60      0.88      0.72      2087\n",
            "           3       0.50      0.24      0.33       324\n",
            "           4       0.58      0.39      0.46       400\n",
            "           5       0.55      0.30      0.39       464\n",
            "\n",
            "    accuracy                           0.60      4368\n",
            "   macro avg       0.58      0.42      0.47      4368\n",
            "weighted avg       0.60      0.60      0.57      4368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost con ajustes de parámetros\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Crear el modelo XGBoost\n",
        "model = XGBClassifier()\n",
        "\n",
        "# Definir los parámetros a ajustar\n",
        "parameters = {'learning_rate': [0.001],\n",
        "              'max_depth': [3],\n",
        "              'n_estimators': [500]}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Obtener la mejor configuración de parámetros encontrada\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Crear el modelo final con los mejores parámetros\n",
        "final_model = XGBClassifier(**best_params)\n",
        "final_model.fit(X_train_vectorized, Y_train_encoded)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "Y_pred_xgb = final_model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(classification_report(Y_test_encoded, Y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0mjaRvRSVxp",
        "outputId": "d70b6a54-9609-4951-d3a8-5e6cdc14b913"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.26      0.39       621\n",
            "           1       0.79      0.07      0.12       472\n",
            "           2       0.52      0.97      0.67      2087\n",
            "           3       0.68      0.15      0.24       324\n",
            "           4       0.41      0.08      0.13       400\n",
            "           5       0.74      0.11      0.19       464\n",
            "\n",
            "    accuracy                           0.54      4368\n",
            "   macro avg       0.66      0.27      0.29      4368\n",
            "weighted avg       0.61      0.54      0.44      4368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"><h1>**Conclusiones**</h1></font>\n",
        "\n",
        "Hay que tener en cuenta que la clase objetivo no está equilibrada, dando lugar a un conjunto de datos desequilibrado.\n",
        "\n",
        "Según los resultados se puede determinar que ***XGBoost*** es el modelo que muestra un desempeño generalmente mejor en términos de precisión, recall y f1-score.\n",
        "\n",
        "Para intentar mejorar los modelos iniciales se ha optado por ajustar los hiperparámetros.\n",
        "\n",
        "\n",
        "1.   SVC : se ha conseguido mejorar ligeramente el modelo.\n",
        "2.   RF : se consigue una mejora muy leve.\n",
        "3.   NB : hay mejora pero los scores conseguidos son justos.\n",
        "4.   XGBoost : en este caso con diferentes configuraciones de los parámetros no mejora el resultado anterior.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PoSHIJVbybv_"
      }
    }
  ]
}